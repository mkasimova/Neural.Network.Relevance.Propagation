{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n",
    "\n",
    "## Load necessary libraries. \n",
    "We suggest you use anaconda to setup a python 2 environment and install the following dependencies:\n",
    " * mdtraj https://anaconda.org/omnia/mdtraj\n",
    " * sklearn https://scikit-learn.org/stable/install.html\n",
    " * bio pandas https://anaconda.org/conda-forge/biopandas\n",
    " \n",
    "Maybe I've missed some library. In that case please add it to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:11:09.269162Z",
     "start_time": "2019-01-29T13:11:08.687366Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(name)s-%(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "import os\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "from modules import utils, feature_extraction as fe, postprocessing, visualization\n",
    "from modules.data_generation import DataGenerator\n",
    "from modules import filtering, data_projection as dp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(\"notebook\")\n",
    "working_dir = \"/media/oliverfl/CHARMANDER/bachelor-thesis-2019/\"  #TODO change directory\n",
    "logger.info(\"Done with init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MD trajectories\n",
    "To use mdtraj, see http://mdtraj.org/latest/examples/introduction.html\n",
    "\n",
    "There are currently **6** different simulations avaialable:\n",
    " * 3 simulations which have an agonist ligand (a drug favouring the active state) bound to the receptor. We call this __holo__\n",
    " \n",
    " \n",
    " * 3 simulations which have no ligand. We call this __apo__\n",
    " \n",
    " \n",
    " * 2 simulations (one holo; one apo) have the conserved residue Asp79 protonated (an extra proton). We call this __ash79__\n",
    " \n",
    " \n",
    " * 2 simulations (one holo; one apo) have Asp79 deprotonated but with a sodium ion forced next to it. We call this __asp79_Na__\n",
    " \n",
    " \n",
    " * 2 simulations (one holo; one apo; you get it now) have Asp79 deprotonated and no sodium force bound. We call this __asp79__. **Start with just these two!**\n",
    " \n",
    "There is more data available in the directory 'longer'. If you need even more, tell me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:35:00.645295Z",
     "start_time": "2019-01-29T13:34:59.733950Z"
    }
   },
   "outputs": [],
   "source": [
    "traj_dir = working_dir + \"string_trajectories/shorter/\" #TODO change directory\n",
    "apo_asp_traj = md.load(traj_dir + \"asp79-apo-swarms-nowater-nolipid.xtc\", \n",
    "                        top=traj_dir + \"asp79-apo-swarms-nowater-nolipid.pdb\") \n",
    "holo_asp_traj = md.load(traj_dir + \"asp79-holo-swarms-nowater-nolipid.xtc\", \n",
    "                        top=traj_dir + \"asp79-holo-swarms-nowater-nolipid.pdb\")  \n",
    "logger.info(\"Loaded trajectories with properties %s, %s\", holo_asp_traj, apo_asp_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: select a subset om the atoms in the trajectory\n",
    "See http://mdtraj.org/latest/examples/atom-selection.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T12:52:16.303353Z",
     "start_time": "2019-01-29T12:52:16.297036Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the interatomic distances for mdtrajectories\n",
    "See http://mdtraj.org/1.6.2/api/generated/mdtraj.compute_contacts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:37:16.138868Z",
     "start_time": "2019-01-29T13:36:16.216446Z"
    }
   },
   "outputs": [],
   "source": [
    "holo_asp_distances, holo_asp_residue_pairs = md.compute_contacts(holo_asp_traj,\n",
    "                                   contacts=\"all\",\n",
    "                                   scheme=\"closest-heavy\", #You may want to use 'ca'\n",
    "                                   ignore_nonprotein=True)\n",
    "apo_asp_distances, apo_asp_residue_pairs = md.compute_contacts(apo_asp_traj,\n",
    "                                   contacts=\"all\",\n",
    "                                   scheme=\"closest-heavy\", #You may want to use 'ca'\n",
    "                                   ignore_nonprotein=True)\n",
    "\n",
    "if holo_asp_distances.shape[1] != apo_asp_distances.shape[1]:\n",
    "    raise Exception(\"Different number of contacts in the two simulations. Must be the same\")\n",
    "logger.info(\"Done computing %s and %s distances\", holo_asp_distances.shape, apo_asp_distances.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge this together into a data format suitable for training - samples and labels\n",
    "\n",
    "Once you're done you want to train classifiers to predict the labels with the samples as input. i.e. based on the contacts in the protein, predict if it was apo or holo etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:39:57.185619Z",
     "start_time": "2019-01-29T13:39:56.927716Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = np.empty((len(holo_asp_distances) + len(apo_asp_distances), apo_asp_distances.shape[1]))\n",
    "labels = np.empty((samples.shape[0],))\n",
    "samples[0:len(holo_asp_distances)] = holo_asp_distances\n",
    "samples[len(holo_asp_distances):] = apo_asp_distances\n",
    "labels[0:len(holo_asp_distances)] = 1 #We label holo with '1' \n",
    "labels[len(holo_asp_distances):] = 2 # and apo with '2' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the indices of the samples to the right residue number\n",
    "\n",
    "Make sure to understand the differnece between the residue index in the topology and the biological residue number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:39:58.116726Z",
     "start_time": "2019-01-29T13:39:57.934164Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_to_resids = np.empty((samples.shape[1], 2)) #This array tells us which residues the index of a certain feature correspond to. \n",
    "for feature_idx, (res1h, res2h) in enumerate(holo_asp_residue_pairs):\n",
    "    res1a, res2a = apo_asp_residue_pairs[feature_idx]\n",
    "    #Convert to biological residue number - this is independent of what your topology (which is a datastructure) looks like\n",
    "    res1h = holo_asp_traj.top.residue(res1h).resSeq\n",
    "    res2h = holo_asp_traj.top.residue(res2h).resSeq\n",
    "    res1a = apo_asp_traj.top.residue(res1a).resSeq\n",
    "    res2a = apo_asp_traj.top.residue(res2a).resSeq    \n",
    "    if res1h != res1a or res2h != res2a:\n",
    "        raise Exception(\"Features differ at index %s. Must be aligned. (%s!=%s or %s!=%s)\", feature_idx, res1h, res1a, res2h, res2a)\n",
    "    else:\n",
    "        feature_to_resids[feature_idx, 0] = res1h\n",
    "        feature_to_resids[feature_idx, 1] = res2h        \n",
    "logger.info(\"Done. Created samples of shape %s\", samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: save these as numpy npy files.\n",
    "Then you can load them again in the future and skip the first steps\n",
    "\n",
    "You can also split this notebook into two separate script - one which computes the distances and one which does feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T12:53:17.710566Z",
     "start_time": "2019-01-29T12:52:20.087Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO samples.save(\"xxxxx.npy) etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "Start by looking at the following 3 methods for classification:\n",
    "\n",
    "** Random Forest (RF)**\n",
    " * Background: https://en.wikipedia.org/wiki/Random_forest\n",
    " * Implementation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    " \n",
    "** Multilayer Perceptron (MLP)** \n",
    " * The most time consuming one (probably)\n",
    " *Background: https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    " * Implementation: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    " \n",
    "** Kullbackâ€“Leibler (KL) Divergence **\n",
    " * see https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:40:00.702240Z",
     "start_time": "2019-01-29T13:40:00.687877Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iterations, n_splits = 1, 1 #Number of times to run and number of splits in cross validation\n",
    "filter_by_distance_cutoff = False #Remove all distances greater than 0.5 nm (configurable limit). Typically residues close to each other contribute most to the stability of the protein\n",
    "use_inverse_distances = True #Usually it is a good idea to take the inverse of the distances since a larg number then indicates two residues in contact -> stronger interaction\n",
    "\n",
    "feature_extractors = [\n",
    "     fe.MlpFeatureExtractor(samples, labels, n_splits=n_splits, n_iterations=n_iterations, \n",
    "                             hidden_layer_sizes=(100,), #You may need to tweak this\n",
    "                             activation=\"relu\", #use \"relu\" or logistic\n",
    "                             randomize=True, # set to false for reproducability\n",
    "                             filter_by_distance_cutoff=filter_by_distance_cutoff),                         \n",
    "     fe.RandomForestFeatureExtractor(samples, labels, n_splits=n_splits, n_iterations=n_iterations,\n",
    "                             filter_by_distance_cutoff=filter_by_distance_cutoff),\n",
    "     fe.KLFeatureExtractor(samples, labels, n_splits=n_splits,\n",
    "                            filter_by_distance_cutoff=filter_by_distance_cutoff),    \n",
    "############ Unsupervised learning methods, skip them for a start#######################\n",
    "#      fe.RbmFeatureExtractor(rbm_data, cluster_indices, n_splits=n_splits, n_iterations=n_iterations, \n",
    "#                           n_components=8,\n",
    "#                            use_inverse_distances=use_inverse_distances,\n",
    "#                           filter_by_distance_cutoff=filter_by_distance_cutoff),    \n",
    "#      fe.PCAFeatureExtractor(data, cluster_indices, n_splits=n_splits,\n",
    "#                            filter_by_distance_cutoff=filter_by_distance_cutoff),    \n",
    "]\n",
    "logger.info(\"Done. using %s feature extractors\", len(feature_extractors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run feature extraction - this may take a little while\n",
    "\n",
    "Every feature extractor will contain the \"raw\" data, the relevance per feature. For KL and MLP you will also get the relevance for apo and holo separately. For MLP you can even get it per frame, but that requires some code change I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:41:27.997328Z",
     "start_time": "2019-01-29T13:40:01.415095Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for extractor in feature_extractors:\n",
    "    logger.info(\"Computing relevance for extractors %s\", extractor.name)\n",
    "    feature_importance, std_feature_importance, errors = extractor.extract_features()\n",
    "    #logger.info(\"Get feature_importance and std of shapes %s, %s\", feature_importance.shape, std_feature_importance.shape)\n",
    "    results.append((extractor, feature_importance, std_feature_importance, errors))\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing\n",
    "\n",
    "Again, you may want to save the relevance as npy files for faster access in the future\n",
    "\n",
    "## Now use the computed relevance and postprocess them to something more useful\n",
    "\n",
    "Once you get the hang of it you might want to write your own code for postprocessing and visualization to get the results you're looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:45:53.358321Z",
     "start_time": "2019-01-29T13:45:47.718823Z"
    }
   },
   "outputs": [],
   "source": [
    "postprocessors = []\n",
    "for (extractor, feature_importance, std_feature_importance, errors) in results:\n",
    "    p = postprocessing.PostProcessor(extractor, feature_importance, std_feature_importance, errors, labels,\n",
    "                                     working_dir  + \"analysis/\", \n",
    "                                     pdb_file=working_dir + \"analysis/apo_only_protein.pdb\", \n",
    "                                     feature_to_resids=feature_to_resids, \n",
    "                                     filter_results=True)\n",
    "    p.average()\n",
    "    p.evaluate_performance()\n",
    "    p.persist()\n",
    "    postprocessors.append([p])\n",
    "logger.info(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T13:45:53.743608Z",
     "start_time": "2019-01-29T13:45:53.368674Z"
    }
   },
   "outputs": [],
   "source": [
    "visualization.visualize(postprocessors,\n",
    "          show_importance=True, \n",
    "          show_performance=False, \n",
    "          show_projected_data=False)\n",
    "\n",
    "logger.info(\"Done. The settings were n_iterations, n_splits = %s, %s.\\nFiltering (filter_by_distance_cutoff = %s)\", \n",
    "            n_iterations, n_splits, filter_by_distance_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "**1) Look at the generated structures and the trajectories** \n",
    " * Spend some time on this and really get used to looking at 3D protein structures. The 'beta' value of each residue should be set to the relevance\n",
    " * Do these results make sense? I.e. the features picked up by the feature extractor, can you see how they change with visual inspection \n",
    " \n",
    " \n",
    "**2) Do more qualitative analysis** \n",
    " * Read last year's bachelor thesis and look at what was done there \n",
    " * What is the efffect of cross validation?\n",
    " * Do scatter plots for different features and see if you can easily make the different conditions cluster together. \n",
    " \n",
    " \n",
    "**3) Summarize work so far**\n",
    " * Write down the conclusions you've made so far\n",
    " * Save the plots and the protein structures for the report. \n",
    " * If there are errors or it doesn't work at all, try and find the reason why\n",
    " \n",
    " \n",
    "**4) Now go back and try to include more than just two conditions**\n",
    " * do multi class classfication, try and separate simulations with deprotonated Asp79 from neutrally charged Asp79 and so on ... \n",
    " * You might not have time to look at everything so think carefully about what the most relevant/interesting questions to ask are based on your results so far. \n",
    " * Iterate through the steps here until you get bored\n",
    " \n",
    "**Optional work**\n",
    " * Try the methods for unsupervised learning, PCA and RBM specifically\n",
    " * Try even more methods for dimensionality reduction and feature extraction. There is a lot available in scikit learn\n",
    " * Use other types of features (dihedral angles, raw xyz coordinates, contacts with different cutoffs etc)  and compare\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "261px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
